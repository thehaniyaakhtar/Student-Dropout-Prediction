{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":4802354,"sourceType":"datasetVersion","datasetId":2780494}],"dockerImageVersionId":31089,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Student Dropout Prediction Analysis\n## Higher Education Predictors of Student Retention\n\nThis notebook analyzes a dataset containing various factors that may influence student retention in higher education. The goal is to predict whether a student will graduate, dropout, or remain enrolled based on multiple features including academic performance, demographic information, and socioeconomic factors.\n\n**Dataset Source**: Kaggle - Higher Education Predictors of Student Retention\n**Target Variable**: Student outcome (Dropout, Graduate, Enrolled)\n**Approach**: Multi-class classification using various machine learning algorithms\n\n---\n\n### Project Overview\nStudent retention is a critical concern for educational institutions. This analysis aims to:\n1. Identify key factors that influence student outcomes\n2. Build predictive models to forecast student success\n3. Provide insights for improving retention strategies\n4. Compare different machine learning approaches","metadata":{}},{"cell_type":"markdown","source":"## 1. Data Loading and Initial Setup\n\n### Import Required Libraries\nWe'll need various libraries for data manipulation, visualization, and machine learning:","metadata":{}},{"cell_type":"code","source":"# Import data manipulation and visualization libraries\nimport pandas as pd\nimport plotly.express as px\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Import machine learning evaluation metrics\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n\n# Import various classification algorithms\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom xgboost import XGBClassifier\nimport sklearn.svm as svm\n\n# Import hyperparameter tuning and ensemble methods\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nfrom sklearn.ensemble import VotingClassifier\n\n# Set style for better visualizations\nplt.style.use('seaborn-v0_8')\nsns.set_palette(\"husl\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-19T08:14:53.087Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Load the Dataset\nLoad the student retention dataset from Kaggle:","metadata":{}},{"cell_type":"code","source":"# Load the dataset containing student information and retention data\ndf = pd.read_csv(\"/kaggle/input/higher-education-predictors-of-student-retention/dataset.csv\")\n\nprint(f\"Dataset loaded successfully!\")\nprint(f\"Shape: {df.shape}\")\nprint(f\"Columns: {len(df.columns)}\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-19T08:14:53.338Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2. Exploratory Data Analysis (EDA)\n\n### Initial Data Inspection\nLet's examine the structure and content of our dataset:","metadata":{}},{"cell_type":"code","source":"# Display the first few rows to understand the data structure\nprint(\"First 5 rows of the dataset:\")\ndf.head()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-19T08:14:53.354Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check data types of all columns\nprint(\"Data types of all columns:\")\ndf.dtypes","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-19T08:14:53.356Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Get comprehensive information about the dataset including missing values\nprint(\"Dataset information:\")\ndf.info()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-19T08:14:53.357Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Target Variable Analysis\nUnderstanding our target variable (student outcome):","metadata":{}},{"cell_type":"code","source":"# Check unique values in the target variable\nprint(\"Unique values in Target variable:\")\ndf['Target'].unique()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-19T08:14:53.363Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Analyze the distribution of target classes\nprint(\"Distribution of target classes:\")\ntarget_counts = df['Target'].value_counts()\nprint(target_counts)\n\n# Calculate percentages\nprint(\"\\nPercentage distribution:\")\nprint((target_counts / len(df) * 100).round(2))","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-19T08:14:53.366Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Target Variable Encoding\nConvert categorical target variable to numerical for machine learning:","metadata":{}},{"cell_type":"code","source":"# Encode target variable: Dropout=0, Graduate=1, Enrolled=2\ntarget_mapping = {\n    'Dropout': 0,\n    'Graduate': 1,\n    'Enrolled': 2\n}\n\ndf['Target'] = df['Target'].map(target_mapping)\nprint(\"Target variable encoded successfully!\")\nprint(f\"Mapping: {target_mapping}\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-19T08:14:53.376Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3. Feature Selection and Correlation Analysis\n\n### Correlation Analysis\nIdentify features most correlated with the target variable:","metadata":{}},{"cell_type":"code","source":"# Calculate correlation between all features and the target variable\ntarget_correlations = df.corr()['Target'].sort_values(ascending=False)\nprint(\"Correlation with Target variable (sorted by absolute value):\")\nprint(target_correlations)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-19T08:14:53.384Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Feature Engineering\nRemove low-correlation features to improve model performance:","metadata":{}},{"cell_type":"code","source":"# Create a cleaned dataset by removing low-correlation features\nfeatures_to_remove = [\n    'GDP', 'Inflation rate', 'Unemployment rate',\n    'Curricular units 2nd sem (without evaluations)',\n    'Curricular units 1st sem (without evaluations)',\n    'Curricular units 2nd sem (credited)',\n    'Curricular units 2nd sem (enrolled)',\n    'International', 'Curricular units 1st sem (credited)',\n    'Curricular units 1st sem (enrolled)',\n    'Application order', 'Course', 'Daytime/evening attendance',\n    'Previous qualification', 'Nacionality',\n    'Educational special needs'\n]\n\ndf1 = df.copy()\ndf1 = df1.drop(columns=features_to_remove)\n\nprint(f\"Original dataset shape: {df.shape}\")\nprint(f\"Cleaned dataset shape: {df1.shape}\")\nprint(f\"Features removed: {len(features_to_remove)}\")\nprint(f\"Features remaining: {len(df1.columns)}\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-19T08:14:53.390Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 4. Data Visualization\n\n### Target Distribution\nVisualize the distribution of student outcomes:","metadata":{}},{"cell_type":"code","source":"# Create an interactive pie chart showing target distribution\ntarget_counts = df1['Target'].value_counts()\nlabels = ['Graduate', 'Dropout', 'Enrolled']\n\nfig = px.pie(\n    values=target_counts.values,\n    names=labels,\n    title='Distribution of Student Outcomes',\n    hole=0.4,\n    color_discrete_sequence=['#2E8B57', '#DC143C', '#4169E1']\n)\n\nfig.update_traces(\n    textinfo='percent+label',\n    pull=[0, 0.2, 0.1],\n    textfont_size=12\n)\n\nfig.update_layout(\n    title_x=0.5,\n    title_font_size=16\n)\n\nfig.show()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-19T08:14:53.393Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Top Features Analysis\nVisualize the most important features based on correlation:","metadata":{}},{"cell_type":"code","source":"# Create a horizontal bar chart of top 10 most correlated features\ncorrelation = df1.corr()['Target'].drop('Target')\ntop_10_features = correlation.abs().nlargest(10).index\ntop_10_corr = correlation[top_10_features]\n\nplt.figure(figsize=(12, 8))\ncolors = ['red' if x < 0 else 'green' for x in top_10_corr]\nbars = plt.barh(range(len(top_10_features)), top_10_corr, color=colors, alpha=0.7)\n\nplt.yticks(range(len(top_10_features)), top_10_features)\nplt.xlabel('Correlation with Target', fontsize=12)\nplt.title('Top 10 Features Most Correlated with Student Outcome', fontsize=14, pad=20)\nplt.grid(axis='x', alpha=0.3)\n\n# Add value labels on bars\nfor i, (bar, value) in enumerate(zip(bars, top_10_corr)):\n    plt.text(value + (0.01 if value > 0 else -0.01), \n             bar.get_y() + bar.get_height()/2, \n             f'{value:.3f}', \n             ha='left' if value > 0 else 'right',\n             va='center',\n             fontweight='bold')\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-19T08:14:53.397Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 5. Model Preparation\n\n### Feature and Target Separation\nPrepare features (X) and target variable (y) for modeling:","metadata":{}},{"cell_type":"code","source":"# Separate features and target variable\nX = df.drop('Target', axis=1)\ny = df['Target']\n\nprint(f\"Features shape: {X.shape}\")\nprint(f\"Target shape: {y.shape}\")\nprint(f\"Number of features: {X.shape[1]}\")\nprint(f\"Number of samples: {X.shape[0]}\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-19T08:14:53.403Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Train-Test Split\nSplit the data into training and testing sets:","metadata":{}},{"cell_type":"code","source":"# Split data into 80% training and 20% testing sets\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, \n    test_size=0.2, \n    random_state=42,\n    stratify=y  # Ensure balanced split across classes\n)\n\nprint(f\"Training set shape: {X_train.shape}\")\nprint(f\"Testing set shape: {X_test.shape}\")\nprint(f\"Training target distribution: {dict(y_train.value_counts())}\")\nprint(f\"Testing target distribution: {dict(y_test.value_counts())}\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-19T08:14:53.430Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 6. Model Training\n\n### Initialize Multiple Classifiers\nSet up various machine learning algorithms for comparison:","metadata":{}},{"cell_type":"code","source":"# Initialize different classification algorithms\nmodels = {\n    'Decision Tree': DecisionTreeClassifier(random_state=0),\n    'Random Forest': RandomForestClassifier(random_state=2),\n    'Logistic Regression': LogisticRegression(max_iter=5000, random_state=42),\n    'K-Nearest Neighbors': KNeighborsClassifier(n_neighbors=3),\n    'AdaBoost': AdaBoostClassifier(n_estimators=50, learning_rate=1, random_state=0),\n    'XGBoost': XGBClassifier(tree_method='hist', device='cuda'),\n    'Support Vector Machine': svm.SVC(kernel='linear', probability=True)\n}\n\nprint(\"Models initialized successfully!\")\nprint(f\"Number of models: {len(models)}\")\nfor name, model in models.items():\n    print(f\"  - {name}\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-19T08:14:53.448Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Train All Models\nFit all models on the training data:","metadata":{}},{"cell_type":"code","source":"# Train all models on the training dataset\nprint(\"Training models...\")\nfor name, model in models.items():\n    print(f\"Training {name}...\")\n    model.fit(X_train, y_train)\n    print(f\"✓ {name} trained successfully!\")\n\nprint(\"\\nAll models trained successfully!\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-19T08:14:53.452Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 7. Model Evaluation\n\n### Individual Model Performance\nEvaluate each model's accuracy on the test set:","metadata":{}},{"cell_type":"code","source":"# Evaluate all models and store results\nresults = {}\n\nprint(\"Model Performance Evaluation:\")\nprint(\"=\" * 50)\n\nfor name, model in models.items():\n    # Make predictions\n    y_pred = model.predict(X_test)\n    \n    # Calculate metrics\n    accuracy = accuracy_score(y_test, y_pred)\n    f1 = f1_score(y_test, y_pred, average='weighted')\n    precision = precision_score(y_test, y_pred, average='weighted')\n    recall = recall_score(y_test, y_pred, average='weighted')\n    \n    # Store results\n    results[name] = {\n        'Accuracy': accuracy,\n        'F1-Score': f1,\n        'Precision': precision,\n        'Recall': recall\n    }\n    \n    print(f\"{name}:\")\n    print(f\"  Accuracy:  {accuracy:.4f} ({accuracy*100:.2f}%)\")\n    print(f\"  F1-Score:  {f1:.4f}\")\n    print(f\"  Precision: {precision:.4f}\")\n    print(f\"  Recall:    {recall:.4f}\")\n    print()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-19T08:14:53.465Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Model Performance Comparison\nVisualize the performance comparison across all models:","metadata":{}},{"cell_type":"code","source":"# Create a comparison plot of model accuracies\nmodel_names = list(results.keys())\naccuracies = [results[name]['Accuracy'] for name in model_names]\n\nplt.figure(figsize=(14, 8))\nbars = plt.bar(model_names, accuracies, color='skyblue', alpha=0.7)\n\n# Add value labels on bars\nfor bar, acc in zip(bars, accuracies):\n    plt.text(bar.get_x() + bar.get_width()/2, \n             bar.get_height() + 0.005, \n             f'{acc:.3f}', \n             ha='center', \n             va='bottom',\n             fontweight='bold')\n\nplt.xlabel('Models', fontsize=12)\nplt.ylabel('Accuracy Score', fontsize=12)\nplt.title('Model Performance Comparison - Accuracy Scores', fontsize=14, pad=20)\nplt.xticks(rotation=45, ha='right')\nplt.ylim(0, 1)\nplt.grid(axis='y', alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n# Find best model\nbest_model_name = max(results.keys(), key=lambda x: results[x]['Accuracy'])\nbest_accuracy = results[best_model_name]['Accuracy']\nprint(f\"\\nBest performing model: {best_model_name} with accuracy: {best_accuracy:.4f} ({best_accuracy*100:.2f}%)\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-19T08:14:53.470Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 8. Ensemble Methods\n\n### Soft Voting Ensemble\nCreate an ensemble using soft voting (probability-based):","metadata":{}},{"cell_type":"code","source":"# Create soft voting ensemble with multiple classifiers\nens1 = VotingClassifier(\n    estimators=[\n        ('rfc', RandomForestClassifier(random_state=2)),\n        ('lr', LogisticRegression(max_iter=5000, random_state=42)),\n        ('abc', AdaBoostClassifier(n_estimators=50, learning_rate=1, random_state=0)),\n        ('xbc', XGBClassifier(tree_method='hist', device='cuda'))\n    ], \n    voting='soft'\n)\n\nprint(\"Soft voting ensemble created with 4 classifiers:\")\nfor name, model in ens1.estimators:\n    print(f\"  - {name}: {type(model).__name__}\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-19T08:14:53.476Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create hard voting ensemble with multiple classifiers\nens2 = VotingClassifier(\n    estimators=[\n        ('rfc', RandomForestClassifier(random_state=2)),\n        ('lr', LogisticRegression(max_iter=5000, random_state=42)),\n        ('abc', AdaBoostClassifier(n_estimators=50, learning_rate=1, random_state=0)),\n        ('xbc', XGBClassifier(tree_method='hist', device='cuda'))\n    ], \n    voting='hard'\n)\n\nprint(\"Hard voting ensemble created with 4 classifiers:\")\nfor name, model in ens2.estimators:\n    print(f\"  - {name}: {type(model).__name__}\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-19T08:14:53.481Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Train both ensemble models\nprint(\"Training ensemble models...\")\nens1.fit(X_train, y_train)\nprint(\"✓ Soft voting ensemble trained!\")\nens2.fit(X_train, y_train)\nprint(\"✓ Hard voting ensemble trained!\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-19T08:14:53.491Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Make predictions with both ensemble methods\ny_pred_soft = ens1.predict(X_test)\ny_pred_hard = ens2.predict(X_test)\n\nprint(\"Predictions made with both ensemble methods!\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-19T08:14:53.497Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Compare ensemble model performances\nsoft_accuracy = accuracy_score(y_test, y_pred_soft)\nhard_accuracy = accuracy_score(y_test, y_pred_hard)\n\nprint(\"Ensemble Model Performance Comparison:\")\nprint(\"=\" * 40)\nprint(f\"Soft Voting Ensemble Accuracy:  {soft_accuracy:.4f} ({soft_accuracy*100:.2f}%)\")\nprint(f\"Hard Voting Ensemble Accuracy:  {hard_accuracy:.4f} ({hard_accuracy*100:.2f}%)\")\n\nif soft_accuracy > hard_accuracy:\n    print(\"\\nSoft voting ensemble performs better!\")\nelif hard_accuracy > soft_accuracy:\n    print(\"\\nHard voting ensemble performs better!\")\nelse:\n    print(\"\\nBoth ensemble methods perform equally!\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-19T08:14:53.498Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 9. Summary and Conclusions\n\n### Key Findings\n- **Best Individual Model**: Random Forest achieved the highest accuracy\n- **Feature Importance**: Academic performance metrics (grades, approvals) show strongest correlation with student outcomes\n- **Age Factor**: Younger students tend to have better outcomes\n- **Financial Factors**: Tuition payment status and scholarship holding are important predictors\n\n### Recommendations\n1. **Early Intervention**: Focus on early academic performance monitoring\n2. **Age-Specific Support**: Implement targeted support for older students\n3. **Financial Aid**: Ensure timely tuition payment processes and expand scholarship programs\n4. **Predictive Analytics**: Use machine learning models for early identification of at-risk students\n\n### Model Performance Summary\nThe analysis shows that ensemble methods and Random Forest provide the best performance for predicting student outcomes. The models can help educational institutions:\n\n- Identify students at risk of dropping out early\n- Allocate resources more effectively\n- Develop targeted intervention strategies\n- Improve overall student retention rates","metadata":{}}]}